defaults:
  - hydra/job_logging: disabled

hydra:
  run:
    dir: ${exp.dir}
  output_subdir: hydra

exp:
  group_name: checks
  name: check_mcs
  version: None
  mode: test   # train or test
  work_dir: ${hydra:runtime.cwd}
  fold_dir: exp/${exp.fold}
  root_dir: ${env:EXPERIMENT_ROOT_DIR}
  group_dir: ${env:EXPERIMENT_ROOT_DIR}/${exp.group_name}
  dir: ${exp.group_dir}/${exp.name}
  version_dir: ${exp.dir}/version_${exp.version}
  fold: 0 # call folds in launcher  via command line
  crossval_n_folds: 10
  crossval_ids_path: ${exp.dir}/crossval_ids.pickle
  raw_output_path: ${exp.version_dir}/raw_output.npy


test:
  name: test_results
  dir: ${exp.dir}/${test.name}
  raw_output_path: ${test.dir}/raw_output.npy
  model_selection: latest # latest or best

data:
  dataset: svhn
  data_dir: ${env:DATASET_ROOT_DIR}/${data.dataset}
  pin_memory: True
  img_size: [32, 32, 3]
  num_workers: 12

model:
  name: small_conv
  n_filters: 32
  fc_dim: 512
  monitor_mcd_samples: 50 # only activated if "mcd" substring in train or val monitor confids.
  test_mcd_samples: 50 # only activated if "mcd" substring in test confids.

trainer:
  num_epochs: 1
  learning_rate: 1e-3
  momentum: 0.9
  weight_decay: 1e-4
  batch_size: 128
  num_classes: 10
  resume_from_ckpt: False
  selection_metric: val/loss    # in logging syntax
  selection_mode: min    # lower is better
  benchmark: True # set to false if input size varies during training!
  global_seed: False # set to False to disable deterministic training.
  val_every_n_epoch: 1
  fast_dev_run: False # True/False/int


augmentations:
  train: # careful, the order here will determine the order of transforms (except normalize will be executed manually at the end after toTensor)
    normalize: [[0.5, 0.5, 0.5],[0.5, 0.5, 0.5]]
  val:
    normalize: [[0.5, 0.5, 0.5],[0.5, 0.5, 0.5]]
  test:
    normalize: [[0.5, 0.5, 0.5],[0.5, 0.5, 0.5]]

eval:
  monitor_metrics: ['accuracy',
                   'failauc',
                   'failap_suc',
                   'failap_err',
                   "mce",
                   "ece",
                   "e-aurc",
                   "aurc",
                   "fpr@95tpr",
                   "default_plot"]
  confidence_measures:
    train:
      ["det_mcp"]
    val:
      ["det_mcp", "det_pe", "mcd_mcp", "mcd_pe", "mcd_ee"]
    test:
      ["det_mcp", "det_pe", "mcd_mcp", "mcd_pe", "mcd_ee"]

  monitor_plots: [ "calibration",
                   "overconfidence",
                   "hist_per_confid" ]

  tb_hparams: ['fold']
