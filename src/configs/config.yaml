defaults:
  - hydra/job_logging: disabled
  - data: default_data.yaml

hydra:
  run:
    dir: ${exp.dir}
  output_subdir: hydra

exp:
  group_name: checks
  name: check_entire_pipeline
  version: None
  mode: train_test   # train or test
  work_dir: ${hydra:runtime.cwd}
  fold_dir: exp/${exp.fold}
  root_dir: ${env:EXPERIMENT_ROOT_DIR}
  group_dir: ${env:EXPERIMENT_ROOT_DIR}/${exp.group_name}
  dir: ${exp.group_dir}/${exp.name}
  version_dir: ${exp.dir}/version_${exp.version}
  fold: 0 # call folds in launcher  via command line
  crossval_n_folds: 10
  crossval_ids_path: ${exp.dir}/crossval_ids.pickle
  raw_output_path: ${exp.version_dir}/raw_output.npy
  external_confids_output_path: ${exp.version_dir}/external_confids.npy
  global_seed: False # set to False to disable deterministic training.



test:
  name: test_results
  dir: ${exp.dir}/${test.name}
  cf_path: ${exp.dir}/hydra/config.yaml
  raw_output_path: ${test.dir}/raw_output.npy
  external_confids_output_path: ${test.dir}/external_confids.npy
  selection_criterion: best_failap_err # model selection criterion or "latest"
  selection_mode: max # model selection criterion or "latest"
  best_ckpt_path: ${exp.version_dir}/${test.selection_criterion}.ckpt # latest or best
  only_latest_version: True # if false looks for best metrics across all versions in exp_dir. Turn to false if resumed training.

model:
  name: confidnet_model #det_mcd_model
  n_filters: 32
  fc_dim: 512
  confidnet_fc_dim: 400
  monitor_mcd_samples: 50 # only activated if "mcd" substring in train or val monitor confids.
  test_mcd_samples: 50 # only activated if "mcd" substring in test confids.
  network:
    name: confidnet_small_conv_and_enc      # confidnet_small_conv_and_enc / small_conv
    backbone: small_conv # weird, the name above should be agnostic then


callbacks:
  model_checkpoint:
    n: 2
    selection_metric: ["val/accuracy", "val/tcp_failap_err"] # in logging syntax: val/loss, makes ure to self.log() this metric! Set to empty to disable. also dont forget confid_prefix.
    mode: ["max", "max"]
    filename: ["best_valacc", "best_failap_err"]# min:lower is better, max: higher is better
    save_top_k: [1, 1]
  confid_monitor:   # not nice: model_checkpoint callback depends on confid_mointor callback
  training_stages:
    milestones: [3, 6]
    pretrained_backbone_path:  #/mnt/hdd2/checkpoints/best.ckpt #  /mnt/hdd2/checkpoints/svhn_smallconv/model_epoch_076.ckpt # leave empty to train backbone on the fly. If set, first milestone needs to be 0!
    pretrained_confidnet_path: #/mnt/hdd2/checkpoints/checks/check_pretrained_confidnet/version_43/best_failap_err.ckpt # leave empty to train confidnet on the fly. If set, second milestone needs to be 0!

trainer:
  num_epochs: 8 # has to be >1 because of incompatibility of lighting eval with psuedo test
  learning_rate: 1e-3
  learning_rate_confidnet: 1e-4
  learning_rate_confidnet_finetune: 1e-7
  momentum: 0.9
  weight_decay: 1e-4
  batch_size: 128
  num_classes: 10
  resume_from_ckpt: False
  benchmark: True # set to false if input size varies during training!
  val_every_n_epoch: 1
  fast_dev_run: False # True/False/int

augmentations:
  train: # careful, the order here will determine the order of transforms (except normalize will be executed manually at the end after toTensor)
    normalize: [[0.5, 0.5, 0.5],[0.5, 0.5, 0.5]]
  val:
    normalize: [[0.5, 0.5, 0.5],[0.5, 0.5, 0.5]]
  test:
    normalize: [[0.5, 0.5, 0.5],[0.5, 0.5, 0.5]]

eval:
  performance_metrics:
    train: ['loss', 'nll', 'accuracy'] # train brier_score logging costs around 5% performance
    val: ['loss', 'nll', 'accuracy', 'brier_score']
    test: ['nll', 'accuracy', 'brier_score']
  confid_metrics:
    train: ['failauc', 'failap_suc', 'failap_err', "fpr@95tpr"]
    val: ['failauc', 'failap_suc', 'failap_err', "fpr@95tpr",  "e-aurc", "aurc"]
    test: ['failauc', 'failap_suc', 'failap_err', "mce", "ece", "e-aurc", "aurc", "fpr@95tpr"]
  confidence_measures: # ["det_mcp" , "det_pe", "tcp" , "mcd_mcp", "mcd_pe", "mcd_ee", "mcd_mi", "mcd_sv"]
    train:
      ["tcp"] # mcd_confs not available due to performance. 'det_mcp' costs around 3% (hard to say more volatile)
    val:
      ["tcp", "det_mcp"] # , "mcd_mcp", "mcd_pe", "mcd_ee", "mcd_mi", "mcd_sv"
    test:
      ["det_mcp" , "det_pe", "tcp", "mcd_mcp", "mcd_pe", "mcd_ee", "mcd_mi", "mcd_sv"]

  monitor_plots: [ #"calibration",
                   #"overconfidence",
                   "hist_per_confid"
                    ]

  tb_hparams: ['fold']
