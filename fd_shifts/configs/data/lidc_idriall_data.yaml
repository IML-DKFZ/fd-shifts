# @package _global_
defaults:
  - override /trainer/lr_scheduler: CosineAnnealingLR
  - override /trainer/optimizer: Adam

data:
  dataset: lidc_idriall
  data_dir: ${oc.env:DATASET_ROOT_DIR}/${data.dataset}
  pin_memory: True
  img_size: [64, 64, 3] #dataset is 28x28x1 either upscale it or need to adjust transforms and neural net
  num_workers: 24
  num_classes: 2
  reproduce_confidnet_splits: True
  target_transforms:
    train:
      extractZeroDim:
    val:
      extractZeroDim:
    test:
      extractZeroDim:  

  augmentations:
    train: # careful, the order here will determine the order of transforms (except normalize will be executed manually at the end after toTensor)
      to_tensor:
      hflip: 1
      rotate: 180
      gaussian_blur:
      rand_erase:
      normalize: [[0.2299,0.2299,0.2299], [0.2402,0.2402,0.2402]]
    val:
      to_tensor:
      normalize: [[0.2299,0.2299,0.2299], [0.2402,0.2402,0.2402]] 
    test:
      to_tensor:
      normalize: [[0.2299,0.2299,0.2299], [0.2402,0.2402,0.2402]]
      

  kwargs:
    
eval:
  query_studies:
    iid_study: ${data.dataset}

trainer:
  batch_size: 512
  # num_epochs: 30
  num_epochs: ${fd_shifts.ifeq_else:${eval.ext_confid_name},tcp,75,${fd_shifts.ifeq_else:${eval.ext_confid_name},dg,60,45}}
  optimizer:
    lr: 3e-5
    weight_decay: 0.1
    # nesterov: False
    # momentum: 0
  num_epochs_backbone: 45
  dg_pretrain_epochs: 45
  val_every_n_epoch: 5

model:
  fc_dim: 1024
  network:
    backbone: densenet121
