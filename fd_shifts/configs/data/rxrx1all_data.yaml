# @package _global_
defaults:
  - override /trainer/lr_scheduler: CosineAnnealingLR
  - override /trainer/optimizer: Adam

data:
  dataset: rxrx1all
  data_dir: ${oc.env:DATASET_ROOT_DIR}/${data.dataset}
  pin_memory: True
  img_size: [512, 512, 6] #dataset is 28x28x1 either upscale it or need to adjust transforms and neural net
  num_workers: 24
  num_classes: 1139
  reproduce_confidnet_splits: True
  target_transforms:
    train:
      extractZeroDim:
    val:
      extractZeroDim:
    test:
      extractZeroDim:  

  augmentations:
    train: # careful, the order here will determine the order of transforms (except normalize will be executed manually at the end after toTensor)
      to_tensor:
      hflip: 1
      randomresized_crop: 224
      rotate: 15
      normalize: [[0.485, 0.456, 0.406,0.485,0.485,0.485], [0.229, 0.224, 0.225,0.229,0.229,0.229]]
    val:
      to_tensor:
      normalize: [[0.485, 0.456, 0.406,0.485,0.485,0.485], [0.229, 0.224, 0.225,0.229,0.229,0.229]] 
    test:
      to_tensor:
      normalize: [[0.485, 0.456, 0.406,0.485,0.485,0.485], [0.229, 0.224, 0.225,0.229,0.229,0.229]]
      

  kwargs:
    
eval:
  performance_metrics:
    test:
      - nll
      - accuracy
      - brier_score
      - b-accuracy
  query_studies:
    iid_study: ${data.dataset}

trainer:
  batch_size: 70
  # num_epochs: 30
  num_epochs: ${fd_shifts.ifeq_else:${eval.ext_confid_name},tcp,150,${fd_shifts.ifeq_else:${eval.ext_confid_name},dg,120,90}}
  optimizer:
    lr: 3e-5
    weight_decay: 1e-5
    # nesterov: False
    # momentum: 0
  num_epochs_backbone: 90
  dg_pretrain_epochs: 90
  val_every_n_epoch: 5

model:
  fc_dim: 2208
  network:
    backbone: densenet161
