# @package _global_
defaults:
  - override /trainer/lr_scheduler: LinearWarmupCosineAnnealingLR

data:
  dataset: breeds
  data_dir: ${oc.env:DATASET_ROOT_DIR}/${data.dataset}
  pin_memory: True
  img_size: [384, 384, 3]
  num_workers: 24
  num_classes: 13 # entitiy-13
  reproduce_confidnet_splits: False
  augmentations:
    train: # careful, the order here will determine the order of transforms (except normalize will be executed manually at the end after toTensor)
      to_tensor:
      resize: 384
      center_crop: 384
      normalize: [[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]]
    val:
      to_tensor:
      resize: 384
      center_crop: 384
      normalize: [[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]]
    test:
      to_tensor:
      resize: 384
      center_crop: 384
      normalize: [[0.485, 0.456, 0.406], [0.229, 0.224, 0.225]] # keep standard norm. corruptions deviate result a bit from training data but ok.

  kwargs:
    info_dir_path: loaders/breeds_hierarchies

eval:
  query_studies:
    iid_study: breeds_384
    in_class_study: 
      - breeds_ood_test_384

trainer:
  num_epochs:
  num_steps: ${fd_shifts.ifeq_else:${eval.ext_confid_name},dg,60000,40000}
  dg_pretrain_epochs:
  dg_pretrain_steps: 20000
  lr_scheduler_interval: step
