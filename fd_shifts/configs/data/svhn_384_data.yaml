# @package _global_
defaults:
  - override /trainer/lr_scheduler: LinearWarmupCosineAnnealingLR

data:
  dataset: svhn
  data_dir: ${oc.env:DATASET_ROOT_DIR}/${data.dataset}
  pin_memory: True
  img_size: [384, 384, 3]
  num_workers: 24
  num_classes: 10
  reproduce_confidnet_splits: True
  augmentations:
    train: # careful, the order here will determine the order of transforms (except normalize will be executed manually at the end after toTensor)
      to_tensor:
      resize: 384
      normalize:
        [[0.4376821, 0.4437697, 0.47280442], [0.19803012, 0.20101562, 0.19703614]]
    val:
      to_tensor:
      resize: 384
      normalize:
        [[0.4376821, 0.4437697, 0.47280442], [0.19803012, 0.20101562, 0.19703614]]
    test:
      to_tensor:
      resize: 384
      normalize:
        [[0.4376821, 0.4437697, 0.47280442], [0.19803012, 0.20101562, 0.19703614]]
  kwargs:

eval:
  query_studies:
    iid_study: svhn_384
    new_class_study:
      - cifar10_384
      - cifar100_384
      - tinyimagenet_384

trainer:
  num_epochs:
  num_steps: ${fd_shifts.ifeq_else:${eval.ext_confid_name},dg,60000,40000}
  dg_pretrain_epochs:
  dg_pretrain_steps: 20000
  lr_scheduler_interval: step
