# @package _global_
defaults:
  - override /trainer/lr_scheduler: CosineAnnealingLR

exp:
  group_name: confidnet
  name: cifar10

trainer:
  num_epochs: 470
  num_epochs_backbone: 250
  optimizer:
    lr: 1e-1
  learning_rate_confidnet: 1e-4
  learning_rate_confidnet_finetune: 1e-6
  lr_scheduler:
    T_max: ${trainer.num_epochs_backbone}
  callbacks:
    model_checkpoint:
    confid_monitor:
    training_stages:
      milestones: [250, 450]
      pretrained_backbone_path:
      pretrained_confidnet_path:
      disable_dropout_at_finetuning: True
      confidnet_lr_scheduler: False
    learning_rate_monitor:

model:
  name: confidnet_model
  fc_dim: 512
  avg_pool: True
  confidnet_fc_dim: 400
  dropout_rate: 1
  monitor_mcd_samples: 50 # only activated if "mcd" substring in train or val monitor confids.
  test_mcd_samples: 50 # only activated if "mcd" substring in test confids.
  network:
    name: confidnet_and_enc # confidnet_small_conv_and_enc / small_conv
    backbone: vgg13
    imagenet_weights_path: #${oc.env:EXPERIMENT_ROOT_DIR}/pretrained_weights/vgg16-397923af.pth

eval:
  ext_confid_name: "tcp"
